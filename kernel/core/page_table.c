#include <page_table.h>
#include <frame_pool.h>
#include <mem_layout.h>

#define ENABLE_MMU		0x00000001
#define ENABLE_DCACHE		0x00000004
#define ENABLE_ICACHE		0x00001000
#define MASK_MMU		0x00000001
#define MASK_DCACHE		0x00000004
#define MASK_ICACHE		0x00001000

#define MMIO_START_ADDR		0xF8000000 /* size : 128MB */

static struct pagetable *pcurrentpgt;
void flush_ent_cache(void);

void init_pageregion(struct pagetable *ppagetable,
		     struct framepool *pframepool,
		     const unsigned int _shared_size)
{
	ppagetable->paging_enabled = 0;
	ppagetable->pframepool = pframepool;
	ppagetable->shared_size = _shared_size;
	ppagetable->ppage_dir = 0;
	ppagetable->ppage_table = 0;
	ppagetable->VMcnt = 0;
}

/* small page translation is used */
void init_pagetable(struct pagetable *ppagetable, PG_TYPE pagetype) 
{
	int i, j;
	int frameno, mmio_s_entry;
	int rd;
	unsigned int *pcur;
	
	if (pagetype == PG_TABLE_KERN && ppagetable->ppage_dir)
		return;

	/* page directory is located in process mem pool 
	 * 16KB for 4096 entry(16KB = 4K * 4Byte) is needed
	 */
	if (pagetype == PG_TABLE_USER) {
		frameno = get_frame(ppagetable->pframepool);
		ppagetable->ppage_dir = (unsigned int *)FRAMETOPHYADDR(frameno);
		/* alloc 3 more contiguous page frames */
		for (i = 0; i < 3; i++) {
			frameno = get_frame(ppagetable->pframepool);
			/* statement with no effect warning. fix me */
			/*FRAMETOPHYADDR(frameno);*/
		}
	} else {
		/* 
		 * assign prealloced 4 contiguous memory frames 
		 * for page_directory : 4K entry 
		 */
		ppagetable->ppage_dir = (unsigned int *)KERN_PGD_START_BASE;
	}

	/* 
	 * each entry of page directory has 1MB memory addressing.
	 * 4G VM address range = 4K Entries * 1MB.
	 * initialize page directory entry as 0. 
	 */
	for (i = 0; i < 4095; i++) {
		ppagetable->ppage_dir[i] = 0;
	}

	if (pagetype == PG_TABLE_KERN) {
		/* 
		 * 6MB directly mapped memory for kernel
		 * 0 ~ 1MB: system reserved
		 * 1 ~ 2MB: kernel text, data, 
		 * 2 ~ 215000KB: stacks
		 * 215000KB ~ 6MB: bitmap, PGD, PGTs
		 * 6 ~ 8MB: kernel heap
		 * 8 ~ 9MB: usr task1 text, data, stack
		 * 9 ~ 12MB: usr task1 PGD, PGTs
		 * ...
		 * ? ~ ?MB: ramdisk img
		 */			
		/* 
		 * preset the 6 entries in page directory 
		 * for kernel with direct mapped address.
		 */
		pcur = (unsigned int *)KERN_PGD_START_BASE;
		for (i = 0; i < 2; i++) { 
			for (j = 0; j < 4; j++) {
				/* 0x11 is
				   Bit[1:0] = 01 : 00:fualt, 01:page, 10:section, 11 : reserved
				   Bit[3:2] = 00 : section : B(ufferable), C(achable), page : don't care
				   Bit[4] = 1
				   Bit[8:5] = 0000 : Domain 0
				   Bit[9] = 0 : don't care
				 */
				ppagetable->ppage_dir[i * 4 + j] = 
					(i << 12) + (((unsigned int)(pcur)+((256 * j) << 2)) | 0x11);
			}
		} 
		/* 0xF8000000 ~0xFFFFFFFF : 128MB, memory mapped I/O 
		 *  memory mapped I/O has direct mapped address
		 */
		mmio_s_entry = MMIO_START_ADDR >> 20;
		pcur = (unsigned int *)(KERN_PGT_START_BASE + (MMIO_START_ADDR >> 10));

		for (i = 0; i < (int)(128 / 4); i++) {
			for (j = 0; j < 4; j++) {
				ppagetable->ppage_dir[mmio_s_entry + (4 * i) + j] = 
					(i << 12) + (((unsigned int)(pcur) + ((256 * j) << 2)) | 0x11);
			}
		}
	}

	if (pagetype == PG_TABLE_KERN) {
		/* 0x002
		 * Bit[1:0] = 10 : small page 
		 * Bit[2] = 0 : Not Bufferable
		 * Bit[3] = 0 : Not Cacheable
		 * Bit[5:4] = Bit[7:6] = Bit[9:8] = Bit[11:10] = 00 
		 * ; the 4KB small page is generated by setting all of 
		 * ; the AP bit pairs to the same values, AP3=AP2=AP1=AP0
		 */

		pcur = (unsigned int *)(ppagetable->ppage_dir[0] & 0xfffffc00);

		/* initialize 8MB(2 * 1024 entry * 4KB) direct mapped memory */
		for (i = 0; i < (int)(8 / 4); i++) { /* 8MB direct pre-mapped memory */
			/*pcur = (unsigned int *)(ppagetable->page_directory[i*4] & 0xfffffc00);*/
			for (j = 0; j < 1024; j++) {
				pcur[i * 1024 + j] = (i * (0x1 << 22) + j * (0x1 << 12)) | 0x002;
			}
		}

		/* initialize mmio region*/
		pcur = (unsigned int *)(ppagetable->ppage_dir[mmio_s_entry] & 0xfffffc00);
		for (i = 0; i < (int)((4096 - mmio_s_entry) / 4); i++) {
			/*pcur = (unsigned int *)(ppagetable->page_directory[i*4] & 0xfffffc00);*/
			for (j = 0; j < 1024; j++) {
				pcur[i * 1024 + j] = MMIO_START_ADDR + (((i * (0x1 << 22) + j * (0x1 << 12))) | 0x002);
			}
		}

		ppagetable->ppage_dir = ppagetable->ppage_dir;
	}

	/* set the domain access register as manager mode
	 *  (access permission not checked) 
	 */
	rd = 0xffffffff; 
	asm ("mcr p15, 0, %0, c3, c0, 0" : : "r" (rd) : );
}

void load_pagetable(struct pagetable *ppagetable)
{
	int r0 = 0;

	pcurrentpgt = ppagetable;
	/* invalidate all TLBs */
	/* invalidate I tlb */
	asm ("mcr p15, 0, %0, c8, c5, 0" : : "r" (r0) :);
	/* invalidate D tlb */
	asm ("mcr p15, 0, %0, c8, c6, 0" : : "r" (r0) :);
	/* invalidate unified tlb */
	asm ("mcr p15, 0, %0, c8, c7, 0" : : "r" (r0) :);

	/* invalidate i cache */
	/*asm ("mcr p15, 0, %0, c7, c5, 1" : : "r"(r0) :);*/
	/* invalidate d cache */
	/*asm ("mcr p15, 0, %0, c7, c6, 1" : : "r"(r0) :);*/

	/* write the translation table base 
	 * currently, the userspace app and kernel uses the same memory space.
	 * TODO : need to split kernel space and user space 
	 */
	/* set TTBCR as 0 for enabling using TTBR0.
	 * Let TTBCR = N>0, if all va[31:32-N] == 0, then TTBR0 is used
	 * otherwise TTBR1 is used, For example Linux N=2, va[00xxxx...b] is userspace,
	 * va[0100...b] ~ va[11....b] is for kernel space.
	 * slos sets the TTBCR as 0 and use TTBR0
	 * PD1(c2[5]) is 0 for enble page table walk in TTB1
	 * PD0(c2[4]) is 0 for enable page table walk in TTB0
	 */
	asm ("mcr p15, 0, %0, c2, c0, 2" : : "r" (r0) :);
	/* set TTBR0 as 0 for depricated user space */
	asm ("mcr p15, 0, %0, c2, c0, 0" : : "r" (ppagetable->ppage_dir) :);
	/* set TTBR1 as kernel page directory base address */
	asm ("mcr p15, 0, %0, c2, c0, 1" : : "r" (ppagetable->ppage_dir) :);
}

void enable_paging()
{
	unsigned int enable = ENABLE_MMU | ENABLE_DCACHE | ENABLE_ICACHE;
	unsigned int mask = MASK_MMU | MASK_DCACHE | MASK_ICACHE;
	unsigned int c1;
	/*int r0 = 0;*/

	/* read control (c1) register of cp 15*/
	asm ("mrc p15, 0, %0, c1, c0, 0" : "=r" (c1) ::);

	/* disable i/d cache */
	/*asm ("mcr p15, 0, %0, c1, c0, 0" : : "r"(c1) :);*/
	/* invalidate entire i cache */
	/*asm ("mcr p15, 0, %0, c7, c5, 0" : : "r"(r0) :);*/
	/* invalidate entire d cache */
	/*asm ("mcr p15, 0, %0, c7, c6, 0" : : "r"(r0) :);*/

	c1 &= ~mask;
	//enable = ENABLE_MMU; // | ENABLE_DCACHE | ENABLE_ICACHE;
	c1 |= enable;
	/* write control register to enable MMU I/D cache */
	asm("mcr p15, 0, %0, c1, c0, 0" : : "r" (c1):);
}

void handle_fault()
{
	int frameno;
	unsigned int *pda;
	unsigned int *pfa;
	unsigned int *pde, *pte, *frame_addr;
	/*unsigned int *page_table;*/
	/*unsigned int *pcur;*/

	/* read DFAR */
	asm ("mrc p15, 0, %0, c6, c0, 0" : "=r" (pfa) ::);
	/* read ttb0 */
	asm ("mrc p15, 0, %0, c2, c0, 0" : "=r" (pda) ::);
#if 0
	/* check fault address is in valid VM region */
	for (i = 0; i < VMcnt; i++) {
		if(pVMref[i]->is_legitimate((unsigned int)pfa)) break;
	}

	/* the fault address is not in valid VM region */
	if(VMcnt != 0 && i == VMcnt) {
		return;
	}
#endif
	/* entry for 1st level descriptor */
	pde = (unsigned int *)((0xffffc000 & (unsigned int)pda) | ((0xfff00000 & (unsigned int)pfa) >> 18));

	/* section fault */
	if (*pde == 0x0) {
		/* page direcoty/table entry in kernel region(<16MB)
		 * are already alloced 
		 * kernel page directory entry : 0x3f8000~0x3fc000
		 * kernel page table entry : 0x400000 ~0x800000
		 */
		if ((unsigned int)pfa < 0x1000000) {
			*pde = KERN_PGT_START_BASE + (((unsigned int)pfa >> 10) | 0x11); /* 256 entry * 4B */
		}
	}

	/* kernel heap fault only */
	frameno = get_frame(pcurrentpgt->pframepool);
	frame_addr = (unsigned int *)FRAMETOPHYADDR(frameno);

	/* entry for 2nd level descriptor */
	pte = (unsigned int *)((*pde & 0xfffffc00) | ((0x000ff000 & (unsigned int)pfa) >> 10));
	/* set the value of 2nd level descriptor */
	*pte = ((unsigned int)frame_addr | 0x002);
}

#if 0
void PageTable::register_vmpool(VMPool *_pool)
{
	pVMref[VMcnt++] = _pool;
}
#endif

void free_page(unsigned int pageAddr)
{
	unsigned int pda, *pde, *pte;
	unsigned int *frame_addr;
	unsigned int frame_num;
	/* read ttb */
	asm ("mrc p15, 0, %0, c2, c0, 0" : "=r" (pda) ::);
	/* get the 1st level descriptor */
	pde = (unsigned int *)((pda & 0xffffc000) | ((pageAddr & 0xfff00000) >> 18));
	pte = (unsigned int *)((*pde & 0xfffffc00) | ((pageAddr & 0x000ff000) >> 10));

	/* physical address of frame */
	frame_addr = (unsigned int *)(*pte);
	frame_num = (unsigned int)(frame_addr) >> 12;

	if (frame_num >= KERNEL_HEAP_START_FRAME &&
 	   frame_num < KERNEL_HEAP_START_FRAME + KERNEL_HEAP_FRAME_NUM) {
		release_frame(pcurrentpgt->pframepool, frame_num);
	} else if (frame_num >= PROCESS_HEAP_START_FRAME &&
		  frame_num < PROCESS_HEAP_START_FRAME + PROCESS_HEAP_FRAME_NUM) {
		release_frame(pcurrentpgt->pframepool, frame_num);
	} else {
		/* error !
		 * only frames in heap can be freed.
		 */
	}

	*pte = 0x0;
	*pde = 0x0;

	/* flush entire cache */
	flush_ent_cache();
}
